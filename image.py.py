# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WypqYphOagr50qX9-7Flm3UdDymDCRqI
"""

import os
import torch
import torchvision
import tarfile
from torchvision.datasets.utils import download_url

# Download the dataset
dataset_url = "http://files.fast.ai/data/cifar10.tgz"
download_url(dataset_url,'.')

# Extract from archive
with tarfile.open('./cifar10.tgz', 'r:gz') as tar:
  def is_within_directory(directory, target):
      
      abs_directory = os.path.abspath(directory)
      abs_target = os.path.abspath(target)
  
      prefix = os.path.commonprefix([abs_directory, abs_target])
      
      return prefix == abs_directory
  
  def safe_extract(tar, path=".", members=None, *, numeric_owner=False):
  
      for member in tar.getmembers():
          member_path = os.path.join(path, member.name)
          if not is_within_directory(path, member_path):
              raise Exception("Attempted Path Traversal in Tar File")
  
      tar.extractall(path, members, numeric_owner=numeric_owner) 
      
  
  safe_extract(tar, path="./data")

data_dir = './data/cifar10'
print(os.listdir(data_dir))
classes = os.listdir(data_dir + "/train")
print(classes)

classes1 = os.listdir(data_dir + "/test")
print(classes1)

airplane_files = os.listdir(data_dir + "/train/airplane")
print("Number of training example for airplane: ", len(airplane_files))
print(airplane_files[:5])

ship_test_file = os.listdir(data_dir + "/test/ship")
print("Number of test example for test: ", len(ship_test_file))
print(ship_test_file[:5])

from torchvision.datasets import ImageFolder
from torchvision.transforms import ToTensor

dataset = ImageFolder(data_dir + '/train', transform=ToTensor())

# printing the sample element from the training dataset.each element is a tupple,containing image tensor and label)
img, label = dataset[3]
print(img.shape,label)
img

print(dataset.classes)

# we can view the image using matplotlib.but we need to  change the tensor dimension to (32,32,3)
import matplotlib.pyplot as plt
def show_example(img, label):
  print('label: ',dataset.classes[label], "("+str(label)+")")
  plt.imshow(img.permute(1,2,0))

show_example(*dataset[0])

show_example(*dataset[5000])

"""**Training and validation dataset**

> 
 **Training dataset**:-used to train the model,compute the loss and adjust the weight of the model using gradient discent


> ***validation dataset***:-used to evalute the model while training, adjust hperparameters(learning raete etc) and pick the best version of model



> **Test dataset**:-used to compare different model anf report the final accuracy of model
"""

import numpy as np
def split_indices(n, val_pct=0.1, seed = 99):
  # determine size of the validation set
  n_val =  int(n*val_pct)
  # set the random seed (for reproductivity)
  np.random.seed(seed)
  # create random permutation of 0 to n-1
  indx = np.random.permutation(n)
  # pick first n_val indices for validation set
  return indx[n_val:], indx[:n_val]

val_pct = 0.2
rand_seed = 42
train_indices, val_indices = split_indices(len(dataset), val_pct, rand_seed)
print(len(train_indices), len(val_indices))
print("simple validation indices ", val_indices[:10])

from torch.utils.data.sampler import SubsetRandomSampler
from torch.utils.data.dataloader import DataLoader
batch_size = 100

# Training sampler and dataloader
train_sampler = SubsetRandomSampler(train_indices)
train_dl = DataLoader(dataset, batch_size, sampler = train_sampler)

# validation sampler and dataloader
val_sampler = SubsetRandomSampler(val_indices)
val_dl = DataLoader(dataset, batch_size, sampler = val_sampler)

from torchvision.utils import make_grid
def show_batch(dl):
  for image, label in dl:
    fig, ax = plt.subplots(figsize=(10,10))
    ax.set_xticks([]); ax.set_yticks([])
    ax.imshow(make_grid(image,10).permute(1,2,0))
    break

show_batch(train_dl)

import torch.nn as nn
import torch.nn.functional as F

simple_model = nn.Sequential(
    nn.Conv2d(3, 8, kernel_size = 3, stride=1, padding=1),
    nn.MaxPool2d(2,2)
)

for images,labels in train_dl:
  print('image.shape: ', images.shape)
  out = simple_model(images)
  print('out.shape: ', out.shape)
  break

model = nn.Sequential(
    nn.Conv2d(3, 16, kernel_size = 3, stride=1, padding=1),
    nn.ReLU(),
    nn.MaxPool2d(2,2), # output bs * 16 * 16 * 16

    
    nn.Conv2d(16, 16, kernel_size = 3, stride=1, padding=1),
    nn.ReLU(),
    nn.MaxPool2d(2,2), # output bs * 16 * 8 * 8

    
    nn.Conv2d(16, 16, kernel_size = 3, stride=1, padding=1),
    nn.ReLU(),
    nn.MaxPool2d(2,2), # output bs * 16 * 4 * 4

    
    nn.Conv2d(16, 16, kernel_size = 3, stride=1, padding=1),
    nn.ReLU(),
    nn.MaxPool2d(2,2), # output bs * 16 * 2 * 2

    
    nn.Conv2d(16, 16, kernel_size = 3, stride=1, padding=1),
    nn.ReLU(),
    nn.MaxPool2d(2,2), # output bs * 16 * 1 * 1

    nn.Flatten(), # output bs*16
    nn.Linear(16,10) # output bs*10
)

def get_default_device():
  # pick gpu if avaliable, else cpu
  if torch.cuda.is_available():
    return torch.device('cuda')
  else:
    return torch.device('cpu')
def to_device(data, service):
  # move tensor to chossen device
  if isinstance(data, (list,tuple)):
    return [to_device(x, device)for x in data]
  return data.to(device, non_blocking = True)

class DeviceDataLoader():
  # wrap a dataloader to move data to a device
  def __init__(self, dl, device):
    self.dl = dl
    self.device = device
  
  def __iter__(self):
    # yield batch of data after moving it to device
    for b in self.dl:
      yield to_device(b, self.device)
  def __len__(self):
    # number of batches
    return len(self.dl)

device = get_default_device()
device

train_dl = DeviceDataLoader(train_dl, device)
valid_dl = DeviceDataLoader(train_dl,device)
to_device(model, device)

"""**Training the model**"""

# Cross-entropy is a measure of the difference between two probability distributions for a given random variable
def loss_batch(model, loss_func, xb, yb, opt = None, metric = None):
  # Generate prediction
  preds = model(xb)
  # calculate loss
  loss = loss_func(preds, yb)

  if opt is not None:
    # compute gradients
    loss.backward()
    # update parameters
    opt.step()
    # reset gradients
    opt.zero_grad()
  metric_result = None
  if matric is not None:
    # compute the metric
    metric_result = metric(pred, yb)

  return loss.item(), len(xb), metric_result

def evaulate(model, loss_fn, valid_dl, metric = None):
  with torch.no_grad():
    # pass each batch through the model
    results = [loss_batch(model, loss_fn, xb, yb, metric=metric)
                  for xb, yb in valid_dl]
    # seperate losses counts and metrices
    losses, nums, metrices = zip(*results)
    # Total size of dataset
    total = np.sum(nums)
    # avg loss across batches
    avg_loss = np.sum(np.multiply(losses,nums))/total
    avf_metric = None
    if metric is not None:
      # avg of metric across batches
      avg_metric = np.sum(np.multiply(metrices,nums))/total
  return avg_loss,total,avg_metric

def fit(epochs, model, loss_fn, train_dl, valid_dl, opt_fn = None, lr = None, metric = None):
  train_losses, val_losses, val_metrices = [],[],[]

  #instantiate the optimizer
  if opt_fn is None: opt_fn = torch.optim.SGD
  opt = opt_fn(model.parameters(),lr = lr)
  
  for epoch in range(epochs):
    #Training 
    model.train()
    for xb,yb in train_dl:
      train_loss,_,_ = loss_batch(model, loss_fn,xb,yb,opt)
    #Evaluation
    model.eval()
    result = evaluate(model, loss_fn, valid_dl, metric)
    val_loss,total,val_metric = result
    # record the loss and metric
    train_losses.append(train_loss)
    val_losses.append(val_loss)
    val_metrices.append(val_metric)

    # print progress
    if metric is None:
      print('Epoch [{}/{}], train_losses: (:4f), val_loss: (:4f)', format(epoch+1, epochs, train_loss, val_loss))
    else:
      
      print('Epoch [{}/{}], train_losses: (:4f), val_loss: (:4f), val_{}: (:4f)', format(epoch+1, epochs, train_loss, val_loss, metric, __name__, val_metric))

  return train_losses,val_losses,val_metrics

def accuracy(outputs, labels):
  _,preds = torch.max(outputs,dim=1)
  return torch.sum(preds==labels).item()/len(preds)

val_loss, _, val_acc = evaulate(model, F.cross_entropy, valid_dl, metric=accuracy)
print('loss: (.4f), accuracy: (.4f)', format(val_loss,val_acc))

